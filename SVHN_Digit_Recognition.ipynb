{"metadata":{"accelerator":"TPU","colab":{"provenance":[],"gpuType":"V28"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Deep Learning Project: Street View Housing Number Digit Recognition**\n\n\n--------------\n## **Context**\n--------------\n\nOne of the most interesting tasks in deep learning is to recognize objects in natural scenes. The ability to process visual information using machine learning algorithms can be very useful as demonstrated in various applications.\n\nThe SVHN dataset contains over 600,000 labeled digits cropped from street-level photos. It is one of the most popular image recognition datasets. It has been used in neural networks created by Google to improve the map quality by automatically transcribing the address numbers from a patch of pixels. The transcribed number with a known street address helps pinpoint the location of the building it represents.\n\n----------------\n## **Objective**\n----------------\n\nOur objective is to predict the number depicted inside the image by using Artificial or Fully Connected Feed Forward Neural Networks and Convolutional Neural Networks. We will go through various models of each and finally select the one that is giving us the best performance.\n\n-------------\n## **Dataset**\n-------------\nHere, we will use a subset of the original data to save some computation time. The dataset is provided as a .h5 file. The basic preprocessing steps have been applied on the dataset.","metadata":{"id":"Q91KqmCRu64D"}},{"cell_type":"markdown","source":"## **Mount the drive**\n\nLet us start by mounting the Google drive. You can run the below cell to mount the Google drive.","metadata":{"id":"8z2Z7-OAs8QG"}},{"cell_type":"code","source":"from google.colab import drive\n\ndrive.mount('/content/drive')","metadata":{"id":"03lDyQUuef7z","executionInfo":{"status":"ok","timestamp":1722912809091,"user_tz":240,"elapsed":16391,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"496dd237-e6d0-4605-8f90-32d3623c353f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Importing the necessary libraries**","metadata":{"id":"C8U3DUa3eNsT"}},{"cell_type":"code","source":"!pip install tensorflow","metadata":{"id":"C-76deapSDB0","executionInfo":{"status":"ok","timestamp":1722912811105,"user_tz":240,"elapsed":2016,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"f6feaa7c-3a5e-4a7e-e04a-36ee7638ef4b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install keras","metadata":{"id":"W6j0p1CFTHL6","executionInfo":{"status":"ok","timestamp":1722912812609,"user_tz":240,"elapsed":1507,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"96e66a9a-783c-4feb-907e-977871387ea6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras\n\nimport numpy as np\n\n# A library for data visualization\nimport matplotlib.pyplot as plt\n\n# An advanced library for data visualization\nimport seaborn as sns\n\nimport tensorflow as tf\n\n# Keras Sequential Model\nfrom tensorflow.keras.models import Sequential\n\n# Importing all the different layers and optimizers\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Activation, LeakyReLU\n\nfrom tensorflow.keras.optimizers import Adam\n\n#from tensorflow.keras.models import load_model\nfrom tensorflow.keras.utils import to_categorical\n\nimport h5py","metadata":{"id":"-dVzeuF3eQx1","executionInfo":{"status":"ok","timestamp":1722912816337,"user_tz":240,"elapsed":3731,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let us check the version of tensorflow.**","metadata":{"id":"ucnevGLoyKf_"}},{"cell_type":"code","source":"print(tf.__version__)","metadata":{"id":"W5as47YxyJVk","executionInfo":{"status":"ok","timestamp":1722912825626,"user_tz":240,"elapsed":138,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"bf5c1868-e969-4c93-8472-e7e531656fbf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Load the dataset**\n\n- Let us now load the dataset that is available as a .h5 file.\n- Split the data into the train and the test dataset.","metadata":{"id":"8lsux2ZwyTTR"}},{"cell_type":"code","source":"# Load the data from the .h5 file\nfile_path = '/content/drive/MyDrive/Colab/Elective_Project/SVHN_single_grey1.h5'\nwith h5py.File(file_path, 'r') as h5_file:\n    X_train = np.array(h5_file['X_train'])\n    y_train = np.array(h5_file['y_train'])\n    X_val = np.array(h5_file['X_val'])\n    y_val = np.array(h5_file['y_val'])\n    X_test = np.array(h5_file['X_test'])\n    y_test = np.array(h5_file['y_test'])","metadata":{"id":"pBjiUk4YV4fL","executionInfo":{"status":"ok","timestamp":1722918082659,"user_tz":240,"elapsed":831,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check the number of images in the training and the testing dataset.","metadata":{"id":"hVe0CYpUgj7w"}},{"cell_type":"code","source":"print(f'Number of training images: {X_train.shape[0]}')\nprint(f'Number of testing images: {X_test.shape[0]}')","metadata":{"id":"y3lwKpOefkpA","executionInfo":{"status":"ok","timestamp":1722918086063,"user_tz":240,"elapsed":156,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"ab5ec112-248a-45a2-c5cf-3ff8e00540cf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.shape","metadata":{"id":"_U-D6h7eYW7x","executionInfo":{"status":"ok","timestamp":1722918089218,"user_tz":240,"elapsed":159,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"4cc31939-9729-4b13-be9d-df6f4e3bf442"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:**","metadata":{"id":"akTUOfLlgwoM"}},{"cell_type":"markdown","source":"## **Visualizing images**\n\n- Use X_train to visualize the first 10 images.\n- Use Y_train to print the first 10 labels.","metadata":{"id":"kxODV6HKykuc"}},{"cell_type":"code","source":"# Visualize the first 10 images and print the first 10 labels\nfig, axes = plt.subplots(1, 10, figsize=(20, 2))\nfor i in range(10):\n    axes[i].imshow(X_train[i], cmap='gray')\n    axes[i].axis('off')\n    axes[i].set_title(f'Label: {y_train[i]}')\n\nplt.show()\n\n# Print the first 10 labels\nprint(\"First 10 labels: \", y_train[:10])","metadata":{"id":"Bvsc8ytHsqWD","executionInfo":{"status":"ok","timestamp":1722909515212,"user_tz":240,"elapsed":634,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"1d0182e2-cf14-4d33-f366-0c99ad55183f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Data preparation**\n\n- Print the shape and the array of pixels for the first image in the training dataset.\n- Normalize the train and the test dataset by dividing by 255.\n- Print the new shapes of the train and the test dataset.\n- One-hot encode the target variable.","metadata":{"id":"kzoyeXHOy80N"}},{"cell_type":"code","source":"# Print the shape and array of pixels for the first image in the training dataset\nprint(f'Shape of the first image: {X_train[0].shape}')\nprint('Array of pixels for the first image:')\nprint(X_train[0])","metadata":{"id":"NqndzQXng9rL","executionInfo":{"status":"ok","timestamp":1722911426861,"user_tz":240,"elapsed":134,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"a18802d5-6011-4e91-f313-629062035bbe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Normalize the train and the test data**","metadata":{"id":"f4CQkKtQ0XII"}},{"cell_type":"code","source":"# Normalize the train and test datasets by dividing by 255\nX_train_normalized = X_train.astype('float32') / 255\nX_val_normalized = X_val.astype('float32') / 255\nX_test_normalized = X_test.astype('float32') / 255","metadata":{"id":"q_yUUTp_mUzB","executionInfo":{"status":"ok","timestamp":1722918101009,"user_tz":240,"elapsed":279,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Print the shapes of Training and Test data","metadata":{"id":"YSlYN6pb8kMY"}},{"cell_type":"code","source":"# Print the new shapes of the train and test datasets\nprint(f'New shape of X_train: {X_train_normalized.shape}')\nprint(f'New shape of X_val: {X_val_normalized.shape}')\nprint(f'New shape of X_test: {X_test_normalized.shape}')","metadata":{"id":"t7FSqOpamWkH","executionInfo":{"status":"ok","timestamp":1722918103124,"user_tz":240,"elapsed":142,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"3a12cfd2-0ea5-44d6-9177-f817abee243a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **One-hot encode output**","metadata":{"id":"0uLxXBpz81vk"}},{"cell_type":"code","source":"# One-hot encode the target variable\ny_train_encoded = to_categorical(y_train)\ny_val_encoded = to_categorical(y_val)\ny_test_encoded = to_categorical(y_test)\n\n# Print the shape of the encoded target variables\nprint(f'Shape of y_train_encoded: {y_train_encoded.shape}')\nprint(f'Shape of y_val_encoded: {y_val_encoded.shape}')\nprint(f'Shape of y_test_encoded: {y_test_encoded.shape}')","metadata":{"id":"zL0lYER4sqWw","executionInfo":{"status":"ok","timestamp":1722918106024,"user_tz":240,"elapsed":141,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"80f067be-9fe9-405c-c8ce-fb6632c0b250"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:**\n","metadata":{"id":"ViqPOTquCF76"}},{"cell_type":"markdown","source":"## **Model Building**\n\nNow that we have done the data preprocessing, let's build an ANN model.\n\n### Fix the seed for random number generators","metadata":{"id":"yH-gVrzuByNA"}},{"cell_type":"code","source":"# Fixing the seed for random number generators\nnp.random.seed(42)\n\nimport random\n\nrandom.seed(42)\n\ntf.random.set_seed(42)","metadata":{"id":"BcKRwrGn0XIL","executionInfo":{"status":"ok","timestamp":1722918109356,"user_tz":240,"elapsed":138,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Model Architecture**\n- Write a function that returns a sequential model with the following architecture:\n - First hidden layer with **64 nodes and the relu activation** and the **input shape = (1024, )**\n - Second hidden layer with **32 nodes and the relu activation**\n - Output layer with **activation as 'softmax' and number of nodes equal to the number of classes, i.e., 10**\n - Compile the model with the **loss equal to categorical_crossentropy, optimizer equal to Adam(learning_rate = 0.001), and metric equal to 'accuracy'**. Do not fit the model here, just return the compiled model.\n- Call the nn_model_1 function and store the model in a new variable.\n- Print the summary of the model.\n- Fit on the train data with a **validation split of 0.2, batch size = 128, verbose = 1, and epochs = 20**. Store the model building history to use later for visualization.","metadata":{"id":"UJDUoaEj1d6e"}},{"cell_type":"markdown","source":"### **Build and train an ANN model as per the above mentioned architecture.**","metadata":{"id":"A48z6ucF0XIP"}},{"cell_type":"code","source":"# Define the ANN model function\ndef nn_model_1(input_shape=(1024,), num_classes=10):\n    model = Sequential()\n\n    # First hidden layer\n    model.add(Dense(64, activation='relu', input_shape=input_shape))\n\n    # Second hidden layer\n    model.add(Dense(32, activation='relu'))\n\n    # Output layer\n    model.add(Dense(num_classes, activation='softmax'))\n\n    # Compile the model\n    model.compile(optimizer=Adam(learning_rate=0.001),\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\n    return model\n\n# Call the function and store the output in a new variable\nann_model_1 = nn_model_1()\n\n# Print the summary of the model\nann_model_1.summary()\n\n# Assuming X_train_ann and y_train_ann are your flattened and one-hot encoded training data\n# Flatten and one-hot encode the target variable if needed\n# Reshape the data\nX_train_flattened = X_train_normalized.reshape((X_train_normalized.shape[0], -1))\nX_val_flattened = X_val_normalized.reshape((X_val_normalized.shape[0], -1))\nX_test_flattened = X_test_normalized.reshape((X_test_normalized.shape[0], -1))\n\n# Fit the model on the training data\nhistory_ann_1 = ann_model_1.fit(X_train_flattened, y_train_encoded,\n                            validation_split=0.2,\n                            batch_size=128,\n                            epochs=20,\n                            verbose=1)\n\n# Print the model summary\nann_model_1.summary()","metadata":{"id":"Cmi81Gr5sqW-","executionInfo":{"status":"ok","timestamp":1722918750724,"user_tz":240,"elapsed":16512,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"dcb66e01-1712-44c8-f14a-cc1a98edf1f8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Plot the Training and Validation Accuracies and write down your Observations.**","metadata":{"id":"MeF8XSWz0XIU"}},{"cell_type":"code","source":"plt.plot(history_ann_1.history['accuracy'])\n\nplt.plot(history_ann_1.history['val_accuracy'])\n\nplt.title('Model Accuracy')\n\nplt.ylabel('Accuracy')\n\nplt.xlabel('Epoch')\n\nplt.legend(['Train', 'Validation'], loc = 'upper left')\n\n# Display the plot\nplt.show()","metadata":{"id":"lt77zgGMP4yw","executionInfo":{"status":"ok","timestamp":1722918812363,"user_tz":240,"elapsed":342,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"d2f08d93-5735-472e-d575-5cc73b3509d7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n\n- The training accuracy continues to improve steadily and reaches a **high value around 0.7**. The validation accuracy also improves and stays close to the training accuracy throughout the epochs, indicating **good generalization**.\n\n- The lines for training and validation accuracy are close to each other and stabilize around the same value towards the end of the training period, suggesting that the model is **not overfitting** and is **generalizing well** to the validation data.\n\n- Room for Improvement - While the model shows steady improvement, the final accuracy around 70% indicates that there might still be room for improvement. Techniques such as adding more layers, experimenting with different activation functions, or tuning hyperparameters could help improve performance further.","metadata":{"id":"pGBbQpLONX7k"}},{"cell_type":"markdown","source":"Let's build one more model with higher complexity and see if we can improve the performance of the model.\n\nFirst, we need to clear the previous model's history from the Keras backend. Also, let's fix the seed again after clearing the backend.","metadata":{"id":"z0qgLMBZm5-K"}},{"cell_type":"code","source":"# Clearing the backend\nfrom tensorflow.keras import backend\n\nbackend.clear_session()\n\n# Fixing the seed for random number generators\nnp.random.seed(42)\n\nimport random\n\nrandom.seed(42)\n\ntf.random.set_seed(42)","metadata":{"id":"I_ih3wEU9wIk","executionInfo":{"status":"ok","timestamp":1722918899801,"user_tz":240,"elapsed":132,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Second Model Architecture**\n- Write a function that returns a sequential model with the following architecture:\n - First hidden layer with **256 nodes and the relu activation** and the **input shape = (1024, )**\n - Second hidden layer with **128 nodes and the relu activation**\n - Add the **Dropout layer with the rate equal to 0.2**\n - Third hidden layer with **64 nodes and the relu activation**\n - Fourth hidden layer with **64 nodes and the relu activation**\n - Fifth hidden layer with **32 nodes and the relu activation**\n - Add the **BatchNormalization layer**\n - Output layer with **activation as 'softmax' and number of nodes equal to the number of classes, i.e., 10**\n -Compile the model with the **loss equal to categorical_crossentropy, optimizer equal to Adam(learning_rate = 0.0005), and metric equal to 'accuracy'**. Do not fit the model here, just return the compiled model.\n- Call the nn_model_2 function and store the model in a new variable.\n- Print the summary of the model.\n- Fit on the train data with a **validation split of 0.2, batch size = 128, verbose = 1, and epochs = 30**. Store the model building history to use later for visualization.","metadata":{"id":"lT6o3TIKuCtk"}},{"cell_type":"markdown","source":"### **Build and train the new ANN model as per the above mentioned architecture**","metadata":{"id":"f-ZjNBmH0XIV"}},{"cell_type":"code","source":"def nn_model_2(input_shape=(1024,), num_classes=10):\n    model = Sequential()\n\n    # First hidden layer\n    model.add(Dense(256, activation='relu', input_shape=input_shape))\n\n    # Second hidden layer\n    model.add(Dense(128, activation='relu'))\n\n    # Dropout layer with rate 0.2\n    model.add(Dropout(0.2))\n\n    # Third hidden layer\n    model.add(Dense(64, activation='relu'))\n\n    # Fourth hidden layer\n    model.add(Dense(64, activation='relu'))\n\n    # Fifth hidden layer\n    model.add(Dense(32, activation='relu'))\n\n    # BatchNormalization layer\n    model.add(BatchNormalization())\n\n    # Output layer\n    model.add(Dense(num_classes, activation='softmax'))\n\n    # Compile the model\n    model.compile(optimizer=Adam(learning_rate=0.0005),\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\n    return model\n\n# Call the function and store the output in a new variable\nann_model_2 = nn_model_2()\n\n# Print the summary of the model\nann_model_2.summary()\n\n# Fit the model on the training data\nhistory_ann_2 = ann_model_2.fit(X_train_flattened, y_train_encoded,\n                                validation_split=0.2,\n                                batch_size=128,\n                                epochs=30,\n                                verbose=1)\n\n# Print the model summary\nann_model_2.summary()","metadata":{"id":"EEPYLFIPnSDP","executionInfo":{"status":"ok","timestamp":1722918949289,"user_tz":240,"elapsed":45515,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"02b95a36-ffb7-49b8-882f-678feab02c3c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Plot the Training and Validation Accuracies and write down your Observations.**","metadata":{"id":"ZJYsvjmw0XIX"}},{"cell_type":"code","source":"plt.plot(history_ann_2.history['accuracy'])\n\nplt.plot(history_ann_2.history['val_accuracy'])\n\nplt.title('Model Accuracy')\n\nplt.ylabel('Accuracy')\n\nplt.xlabel('Epoch')\n\nplt.legend(['Train', 'Validation'], loc = 'upper left')\n\n# Display the plot\nplt.show()","metadata":{"id":"01ig6BrF1KVy","executionInfo":{"status":"ok","timestamp":1722919173087,"user_tz":240,"elapsed":443,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"52fe7aed-dbd4-46cb-9171-d184441bed3d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n\n- The model shows **good performance** with both training and validation accuracy improving significantly and converging around **77%**.\n- **No Overfitting** - The closeness of the training and validation accuracy curves suggests that the model is not overfitting and is **generalizing well** to the validation data.\n- Steady Improvement - The accuracy continues to improve steadily across the epochs, indicating that the model is learning effectively.","metadata":{"id":"VPW1LlD61RDn"}},{"cell_type":"markdown","source":"## **Predictions on the test data**\n\n- Make predictions on the test set using the second model.\n- Print the obtained results using the classification report and the confusion matrix.\n- Final observations on the obtained results.","metadata":{"id":"8kuXx9Bvu00f"}},{"cell_type":"code","source":"# Make predictions on the test set\ny_test_pred = ann_model_2.predict(X_test_flattened)","metadata":{"id":"xbWMEtTj5Ad0","executionInfo":{"status":"ok","timestamp":1722919592906,"user_tz":240,"elapsed":1425,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"4bbdad88-27e0-4f8a-97f9-e1169dfe5038"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Note:** Earlier, we noticed that each entry of the target variable is a one-hot encoded vector but to print the classification report and confusion matrix, we must convert each entry of y_test to a single label.","metadata":{"id":"i3li8Ib08yts"}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\n\n# Convert predictions from one-hot encoded to class labels\ny_test_pred_classes = np.argmax(y_test_pred, axis=1)\ny_test_true_classes = np.argmax(y_test_encoded, axis=1)","metadata":{"id":"NByu7uAQ8x9P","executionInfo":{"status":"ok","timestamp":1722919603396,"user_tz":240,"elapsed":142,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Print the classification report and the confusion matrix for the test predictions. Write your observations on the final results.**","metadata":{"id":"1_SIoopr0XIg"}},{"cell_type":"code","source":"# Print the classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_test_true_classes, y_test_pred_classes))\n\n# Print the confusion matrix\nconf_matrix = confusion_matrix(y_test_true_classes, y_test_pred_classes)\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(10), yticklabels=range(10))\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"id":"xRddeJ-3EHT1","executionInfo":{"status":"ok","timestamp":1722919610056,"user_tz":240,"elapsed":1261,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"57eb85e9-eb41-4313-8239-d2bf0d22a292"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Final Observations:**\n\n- The model shows good precision, recall, and F1-scores for all classes, indicating a **balanced performance overall**. The accuracy of **0.78** suggests that the model correctly classifies **78%** of the instances.\n- Confusion Matrix Insights - Most predictions are **along the diagonal**, indicating **correct classifications**. Some confusion exists between certain classes, such as class 3 being confused with class 4 and class 8.\n- Areas for Improvement- The slightly lower recall for some classes (e.g., class 3 and class 8) suggests room for improvement in correctly identifying these classes. Techniques like data augmentation, additional training data, or more complex architectures might help improve performance further.","metadata":{"id":"DjErl4GA2u9s"}},{"cell_type":"markdown","source":"## **Using Convolutional Neural Networks**","metadata":{"id":"xkR4JioMsuIV"}},{"cell_type":"markdown","source":"### **Load the dataset again and split the data into the train and the test dataset.**","metadata":{"id":"YN2YgkGL_6xQ"}},{"cell_type":"code","source":"# Load the data from the .h5 file\nfile_path = '/content/drive/MyDrive/Colab/Elective_Project/SVHN_single_grey1.h5'\nwith h5py.File(file_path, 'r') as h5_file:\n    X_train = np.array(h5_file['X_train'])\n    y_train = np.array(h5_file['y_train'])\n    X_val = np.array(h5_file['X_val'])\n    y_val = np.array(h5_file['y_val'])\n    X_test = np.array(h5_file['X_test'])\n    y_test = np.array(h5_file['y_test'])","metadata":{"id":"mqM204HbAjP2","scrolled":true,"executionInfo":{"status":"ok","timestamp":1722912841222,"user_tz":240,"elapsed":6774,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check the number of images in the training and the testing dataset.","metadata":{"id":"6fPqF_xGAjQB"}},{"cell_type":"code","source":"print(f'Number of training images: {X_train.shape[0]}')\nprint(f'Number of testing images: {X_test.shape[0]}')","metadata":{"id":"gTLJZWjPAjQB","executionInfo":{"status":"ok","timestamp":1722912845515,"user_tz":240,"elapsed":130,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"a09bc452-bf12-4da9-ca74-02248c41240d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:**\n","metadata":{"id":"9qyMiP_rAjQB"}},{"cell_type":"markdown","source":"## **Data preparation**\n\n- Print the shape and the array of pixels for the first image in the training dataset.\n- Reshape the train and the test dataset because we always have to give a 4D array as input to CNNs.\n- Normalize the train and the test dataset by dividing by 255.\n- Print the new shapes of the train and the test dataset.\n- One-hot encode the target variable.","metadata":{"id":"OJndFfEVAjQG"}},{"cell_type":"code","source":"# Print the shape and array of pixels for the first image in the training dataset\nprint(f'Shape of the first image: {X_train[0].shape}')\nprint('Array of pixels for the first image:')\nprint(X_train[0])","metadata":{"id":"W4uXqKz1AjQG","executionInfo":{"status":"ok","timestamp":1722912849609,"user_tz":240,"elapsed":131,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"3d8ec775-2097-4a43-eeb2-5b363fcfd642"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reshape the dataset to be able to pass them to CNNs. Remember that we always have to give a 4D array as input to CNNs","metadata":{"id":"at30iiX1__7F"}},{"cell_type":"code","source":"# Reshape the data to 4D (samples, height, width, channels)\nX_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))\nX_val = X_val.reshape((X_val.shape[0], X_val.shape[1], X_val.shape[2], 1))\nX_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2], 1))","metadata":{"id":"D9YPwf9ysqWU","executionInfo":{"status":"ok","timestamp":1722912997790,"user_tz":240,"elapsed":113,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Normalize inputs from 0-255 to 0-1","metadata":{"id":"ODYnoLfaAEGx"}},{"cell_type":"code","source":"# Normalize the train and test datasets by dividing by 255\nX_train_normalized = X_train.astype('float32') / 255\nX_val_normalized = X_val.astype('float32') / 255\nX_test_normalized = X_test.astype('float32') / 255","metadata":{"id":"eOGLAn40AjQG","executionInfo":{"status":"ok","timestamp":1722913019199,"user_tz":240,"elapsed":297,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Print New shape of Training and Test","metadata":{"id":"cS9T4HqjAoyM"}},{"cell_type":"code","source":"# Print the new shapes of the train and test datasets\nprint(f'New shape of X_train: {X_train_normalized.shape}')\nprint(f'New shape of X_val: {X_val_normalized.shape}')\nprint(f'New shape of X_test: {X_test_normalized.shape}')","metadata":{"id":"5qf8S5NQAjQG","executionInfo":{"status":"ok","timestamp":1722913035981,"user_tz":240,"elapsed":132,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"12f664cc-8627-4cb9-87b3-91bce6571437"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **One-hot encode the labels in the target variable y_train and y_test.**","metadata":{"id":"10QaOV-xR7Jn"}},{"cell_type":"code","source":"# One-hot encode the target variable\ny_train_encoded = to_categorical(y_train)\ny_val_encoded = to_categorical(y_val)\ny_test_encoded = to_categorical(y_test)\n\n# Print the shape of the encoded target variables\nprint(f'Shape of y_train_encoded: {y_train_encoded.shape}')\nprint(f'Shape of y_val_encoded: {y_val_encoded.shape}')\nprint(f'Shape of y_test_encoded: {y_test_encoded.shape}')","metadata":{"id":"3KHWFWKMAjQH","executionInfo":{"status":"ok","timestamp":1722913069721,"user_tz":240,"elapsed":133,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"a25a59a0-a183-4342-e6ca-fae784440fc7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:**\n","metadata":{"id":"H-8jYVQTAjQH"}},{"cell_type":"markdown","source":"## **Model Building**\n\nNow that we have done data preprocessing, let's build a CNN model.\nFix the seed for random number generators","metadata":{"id":"Vjx_LI4_AjQH"}},{"cell_type":"code","source":"# Fixing the seed for random number generators\nnp.random.seed(42)\n\nimport random\n\nrandom.seed(42)\n\ntf.random.set_seed(42)","metadata":{"id":"ZY5pyF4-KDNt","executionInfo":{"status":"ok","timestamp":1722913104257,"user_tz":240,"elapsed":144,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Model Architecture**\n- **Write a function** that returns a sequential model with the following architecture:\n - First Convolutional layer with **16 filters and the kernel size of 3x3**. Use the **'same' padding** and provide the **input shape = (32, 32, 1)**\n - Add a **LeakyRelu layer** with the **slope equal to 0.1**\n - Second Convolutional layer with **32 filters and the kernel size of 3x3 with 'same' padding**\n - Another **LeakyRelu** with the **slope equal to 0.1**\n - A **max-pooling layer** with a **pool size of 2x2**\n - **Flatten** the output from the previous layer\n - Add a **dense layer with 32 nodes**\n - Add a **LeakyRelu layer with the slope equal to 0.1**\n - Add the final **output layer with nodes equal to the number of classes, i.e., 10** and **'softmax' as the activation function**\n - Compile the model with the **loss equal to categorical_crossentropy, optimizer equal to Adam(learning_rate = 0.001), and metric equal to 'accuracy'**. Do not fit the model here, just return the compiled model.\n- Call the function cnn_model_1 and store the output in a new variable.\n- Print the summary of the model.\n- Fit the model on the training data with a **validation split of 0.2, batch size = 32, verbose = 1, and epochs = 20**. Store the model building history to use later for visualization.","metadata":{"id":"1JUAczhzAjQH"}},{"cell_type":"markdown","source":"### **Build and train a CNN model as per the above mentioned architecture.**","metadata":{"id":"JWsAd45JKDNu"}},{"cell_type":"code","source":"def cnn_model_1(input_shape=(32, 32, 1), num_classes=10):\n    model = Sequential()\n\n    # First Convolutional layer\n    model.add(Conv2D(16, (3, 3), padding='same', input_shape=input_shape))\n    model.add(LeakyReLU(alpha=0.1))\n\n    # Second Convolutional layer\n    model.add(Conv2D(32, (3, 3), padding='same'))\n    model.add(LeakyReLU(alpha=0.1))\n\n    # Max-pooling layer\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n\n    # Flatten the output\n    model.add(Flatten())\n\n    # Dense layer with 32 nodes\n    model.add(Dense(32))\n    model.add(LeakyReLU(alpha=0.1))\n\n    # Final output layer\n    model.add(Dense(num_classes, activation='softmax'))\n\n    # Compile the model\n    model.compile(optimizer=Adam(learning_rate=0.001),\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\n    return model\n\n# Call the function and store the output in a new variable\ncnn_model_1 = cnn_model_1()\n\n# Print the model summary\ncnn_model_1.summary()\n\n# Fit the model on the training data\nhistory_1 = cnn_model_1.fit(X_train_normalized, y_train_encoded,\n                        validation_split=0.2,\n                        batch_size=32,\n                        verbose=1,\n                        epochs=20)","metadata":{"id":"L1jOYANWAjQH","executionInfo":{"status":"ok","timestamp":1722917282916,"user_tz":240,"elapsed":201136,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"24988469-38ac-4006-8fbc-4c52963de626"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Plot the Training and Validation Accuracies and Write your observations.**","metadata":{"id":"JPzfIf9kKDNw"}},{"cell_type":"code","source":"plt.plot(history_1.history['accuracy'])\n\nplt.plot(history_1.history['val_accuracy'])\n\nplt.title('Model Accuracy')\n\nplt.ylabel('Accuracy')\n\nplt.xlabel('Epoch')\n\nplt.legend(['Train', 'Validation'], loc = 'upper left')\n\n# Display the plot\nplt.show()","metadata":{"id":"07oUCr1kAjQH","executionInfo":{"status":"ok","timestamp":1722917605634,"user_tz":240,"elapsed":382,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"028d65f2-e837-45ba-8fcd-2e0a8ba20bee"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n\n- We can see from the above plot that the model has done poorly on the validation data. The model is highly overfitting the training data.\n- The validation accuracy has become more or less constant after 2 epochs.","metadata":{"id":"P6zTLyp9AjQH"}},{"cell_type":"markdown","source":"Let's build another model and see if we can get a better model with generalized performance.\n\nFirst, we need to clear the previous model's history from the Keras backend. Also, let's fix the seed again after clearing the backend.","metadata":{"id":"Ukvtg2eMAjQH"}},{"cell_type":"code","source":"# Clearing the backend\nfrom tensorflow.keras import backend\n\nbackend.clear_session()\n\n# Fixing the seed for random number generators\nnp.random.seed(42)\n\nimport random\n\nrandom.seed(42)\n\ntf.random.set_seed(42)","metadata":{"id":"HbKi93HTolGW","executionInfo":{"status":"ok","timestamp":1722917611105,"user_tz":240,"elapsed":134,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Second Model Architecture**\n\n- Write a function that returns a sequential model with the following architecture:\n - First Convolutional layer with **16 filters and the kernel size of 3x3**. Use the **'same' padding** and provide the **input shape = (32, 32, 1)**\n - Add a **LeakyRelu layer** with the **slope equal to 0.1**\n - Second Convolutional layer with **32 filters and the kernel size of 3x3 with 'same' padding**\n - Add **LeakyRelu** with the **slope equal to 0.1**\n - Add a **max-pooling layer** with a **pool size of 2x2**\n - Add a **BatchNormalization layer**\n - Third Convolutional layer with **32 filters and the kernel size of 3x3 with 'same' padding**\n - Add a **LeakyRelu layer with the slope equal to 0.1**\n - Fourth Convolutional layer **64 filters and the kernel size of 3x3 with 'same' padding**\n - Add a **LeakyRelu layer with the slope equal to 0.1**\n - Add a **max-pooling layer** with a **pool size of 2x2**\n - Add a **BatchNormalization layer**\n - **Flatten** the output from the previous layer\n - Add a **dense layer with 32 nodes**\n - Add a **LeakyRelu layer with the slope equal to 0.1**\n - Add a **dropout layer with the rate equal to 0.5**\n - Add the final **output layer with nodes equal to the number of classes, i.e., 10** and **'softmax' as the activation function**\n - Compile the model with the **categorical_crossentropy loss, adam optimizers (learning_rate = 0.001), and metric equal to 'accuracy'**. Do not fit the model here, just return the compiled model.\n- Call the function cnn_model_2 and store the model in a new variable.\n- Print the summary of the model.\n- Fit the model on the train data with a **validation split of 0.2, batch size = 128, verbose = 1, and epochs = 30**. Store the model building history to use later for visualization.","metadata":{"id":"Ep19Jd8HAjQH"}},{"cell_type":"markdown","source":"### **Build and train the second CNN model as per the above mentioned architecture.**","metadata":{"id":"y5IBLS1eKDNy"}},{"cell_type":"code","source":"# Define the CNN model function\ndef cnn_model_2(input_shape=(32, 32, 1), num_classes=10):\n    model = Sequential()\n\n    # First Convolutional layer\n    model.add(Conv2D(16, (3, 3), padding='same', input_shape=input_shape))\n    model.add(LeakyReLU(alpha=0.1))\n\n    # Second Convolutional layer\n    model.add(Conv2D(32, (3, 3), padding='same'))\n    model.add(LeakyReLU(alpha=0.1))\n\n    # Max-pooling layer\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(BatchNormalization())\n\n    # Third Convolutional layer\n    model.add(Conv2D(32, (3, 3), padding='same'))\n    model.add(LeakyReLU(alpha=0.1))\n\n    # Fourth Convolutional layer\n    model.add(Conv2D(64, (3, 3), padding='same'))\n    model.add(LeakyReLU(alpha=0.1))\n\n    # Max-pooling layer\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(BatchNormalization())\n\n    # Flatten the output\n    model.add(Flatten())\n\n    # Dense layer with 32 nodes\n    model.add(Dense(32))\n    model.add(LeakyReLU(alpha=0.1))\n\n    # Dropout layer with rate 0.5\n    model.add(Dropout(0.5))\n\n    # Final output layer\n    model.add(Dense(num_classes, activation='softmax'))\n\n    # Compile the model\n    model.compile(optimizer=Adam(learning_rate=0.001),\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\n    return model\n\n# Call the function and store the output in a new variable\ncnn_model_2 = cnn_model_2()\n\n# Print the summary of the model\ncnn_model_2.summary()\n\n# Fit the model on the training data\nhistory_2 = cnn_model_2.fit(X_train_normalized, y_train_encoded,\n                        validation_split=0.2,\n                        batch_size=128,\n                        epochs=30,\n                        verbose=1)\n\n# Print the model summary\ncnn_model_2.summary()","metadata":{"id":"wk9sl2UEAjQH","executionInfo":{"status":"ok","timestamp":1722917909540,"user_tz":240,"elapsed":208550,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"02447589-88a4-410b-9907-fc274f4704d4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Plot the Training and Validation accuracies and write your observations.**","metadata":{"id":"PyhUtMy3KDN1"}},{"cell_type":"code","source":"plt.plot(history_2.history['accuracy'])\n\nplt.plot(history_2.history['val_accuracy'])\n\nplt.title('Model Accuracy')\n\nplt.ylabel('Accuracy')\n\nplt.xlabel('Epoch')\n\nplt.legend(['Train', 'Validation'], loc = 'upper left')\n\n# Display the plot\nplt.show()","metadata":{"id":"YVQu7uWiAjQH","executionInfo":{"status":"ok","timestamp":1722917909749,"user_tz":240,"elapsed":210,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"42852ab2-7ec9-4f72-f719-151afb87339c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:**\n\n- The second iteration of this model seems very promising now.\n- **The validation accuracy has improved substantially** and the problem of **overfitting has been solved**. We can say that the **model is giving a generalized performance.**\n- As training progresses, the training accuracy continues to improve steadily and reaches a **high value around 0.9**.\nThe validation accuracy also improves but starts to **stabilize around 0.85**.\n- The gap between the training and validation accuracy suggests some level of overfitting. The model performs very well on the training data but slightly less so on the validation data. Overfitting is common in machine learning models and indicates that the model might be learning the training data too well, including noise and details that do not generalize to new, unseen data.","metadata":{"id":"Qrrt0Ac3AjQH"}},{"cell_type":"markdown","source":"## **Predictions on the test data**\n\n- Make predictions on the test set using the second model.\n- Print the obtained results using the classification report and the confusion matrix.\n- Final observations on the obtained results.","metadata":{"id":"kja4SnOdAjQI"}},{"cell_type":"markdown","source":"### **Make predictions on the test data using the second model.**","metadata":{"id":"eHCRwRbgKDN2"}},{"cell_type":"code","source":"# Make predictions on the test set\ny_test_pred = cnn_model_2.predict(X_test_normalized)","metadata":{"id":"f1d-VvaLAjQI","executionInfo":{"status":"ok","timestamp":1722917912647,"user_tz":240,"elapsed":2898,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"ade49fc6-7f8f-4187-e06b-b48ea9042d05"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Note:** Earlier, we noticed that each entry of the target variable is a one-hot encoded vector, but to print the classification report and confusion matrix, we must convert each entry of y_test to a single label.","metadata":{"id":"DrV1tOG0AjQI"}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\n\n# Convert predictions from one-hot encoded to class labels\ny_test_pred_classes = np.argmax(y_test_pred, axis=1)\ny_test_true_classes = np.argmax(y_test_encoded, axis=1)\n\n# Print the classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_test_true_classes, y_test_pred_classes))\n\n# Print the confusion matrix\nconf_matrix = confusion_matrix(y_test_true_classes, y_test_pred_classes)\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(10), yticklabels=range(10))\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"id":"dUSHU9W0AjQI","executionInfo":{"status":"ok","timestamp":1722917913196,"user_tz":240,"elapsed":549,"user":{"displayName":"Sophia Liang","userId":"11881342219186364728"}},"outputId":"3c3614d7-13f0-42bd-f531-24b7b3bf3bf4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Write your final observations on the performance of the model on the test data.**","metadata":{"id":"aVCa-ysWKDN3"}},{"cell_type":"code","source":"","metadata":{"id":"sOMq2rCJAjQJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Final Observations:**\n\n- The model shows high precision, recall, and F1-scores for all classes, indicating **good performance overall**.\nThe accuracy of **0.91** suggests that the model correctly classifies **91%** of the instances.\n- Confusion Matrix Insights - Most predictions are along the **diagonal**, indicating **correct classifications**. Some confusion exists between certain classes, such as class 1 being confused with class 0 and class 2.\n- Areas for Improvement - The slight drop in recall for some classes (e.g., class 3 and class 6) suggests room for improvement in correctly identifying these classes. Techniques like data augmentation, additional training data, or more complex architectures might help improve performance further.\n- Demonstrated that CNNs are more suitable for image recognition tasks.\n\n","metadata":{"id":"TNN9v713AjQJ"}}]}